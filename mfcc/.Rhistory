load("G:/我的雲端硬碟/Phonotactics/judgement-experiment/distance data/27may-updateddisylcv2.RData")
library(dplyr)
#MFCC
findDisylMFCCDistance = function(disyl){
oldfile = read.table(paste("G:\\我的雲端硬碟\\Phonotactics\\judgement-experiment\\distance audio\\sepaudio\\mfcc\\normalised_MONO-OLD-",disyl,"_mfcc.txt",sep=""))
newfile = read.table(paste("G:\\我的雲端硬碟\\Phonotactics\\judgement-experiment\\distance audio\\sepaudio\\mfcc\\normalised_MONO-NEW-",disyl,"_mfcc.txt",sep=""))
return(spectrumDistance(oldfile,newfile))
}
mfcc_dists_disyl = sapply(1:72,findDisylMFCCDistance)
dist_judgements_melted_di = addDistsToDiTable(mfcc_dists_disyl,"mfcc_acoustic_dist")
mfcc_dists_disyl_2 = mfcc_dists_disyl / max(mfcc_dists_disyl) * 4
dist_judgements_melted_di = addDistsToDiTable(mfcc_dists_disyl_2,"mfcc_acoustic_dist_modified")
model_di_acoust_mfcc = brm(distance | cens(censored) ~ (mfcc_acoustic_dist_modified | participant) +(1|variable) + mfcc_acoustic_dist_modified, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
library(brms)
model_di_acoust_mfcc = brm(distance | cens(censored) ~ (mfcc_acoustic_dist_modified | participant) +(1|variable) + mfcc_acoustic_dist_modified, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
censored_di = as.integer(dist_judgements_melted_di$distance == 8)
dist_judgements_melted_di = cbind.data.frame(dist_judgements_melted_di,censored_di)
dist_judgements_melted_di
model_di_acoust_mfcc = brm(distance | cens(censored) ~ (mfcc_acoustic_dist_modified | participant) +(1|variable) + mfcc_acoustic_dist_modified, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
model_di_acoust_mfcc = brm(distance | cens(censored_di) ~ (mfcc_acoustic_dist_modified | participant) +(1|variable) + mfcc_acoustic_dist_modified, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
model_di_acoust_mfcc
waic(model_di_acoust_mfcc)
may_vs_results = results_wide %>% melt(id.vars = 1:17, measure.vars = 18:648, variable.name = "Question", value.name = "Number") %>% mutate("SC1#1_1":"MP99#1_2" = case_when("1" ~ str_replace(vowel, "1", "AA"),
results_wide %>% melt(id.vars = 1:17, measure.vars = 18:648, variable.name = "Question", value.name = "Number")
library(reshape2)
library(dplyr)
setwd("G:/我的雲端硬碟/May's paper")
results_wide = read.csv("20190822-results.csv")
stimuli = read.csv("Qualtrics_Full_List_of_Stimuli.csv")
results_wide %>% melt(id.vars = 1:17, measure.vars = 18:648, variable.name = "Question", value.name = "Number")
results_wide %>% melt(id.vars = 1:17, measure.vars = 18:648, variable.name = "Question", value.name = "Number")
View(results_wide %>% melt(id.vars = 1:17, measure.vars = 18:648, variable.name = "Question", value.name = "Number") )
View(results_wide)
results_wide = read.csv("20190822-results.csv")[-c(1,2),]
results_wide
View(results_wide)
results_wide[,1:17]
results = results_wide %>% melt(id.vars = 1:17, measure.vars = c(paste0("MT",1:99,".1_1"), paste0("MT",1:99,".1_2"), paste0("MP",1:99,".1_1"), paste0("MP",1:99,".1_2")), variable.name = "Question", value.name = "Number")
results_lr = results_wide %>% melt(id.vars = 1:17, measure.vars = c(paste0("MT",1:99,"LR"), paste0("MP",1:99,"LR"), variable.name = "LR", value.name = "Number")
#mayfailedbadly
results = results_wide %>% melt(id.vars = 1:17, measure.vars = c(paste0("MT",1:99,".1_1"), paste0("MT",1:99,".1_2"), paste0("MP",1:99,".1_1"), paste0("MP",1:99,".1_2")), variable.name = "Question", value.name = "QID")
results_lr = results_wide %>% melt(id.vars = 1:17, measure.vars = c(paste0("MT",1:99,"LR"), paste0("MP",1:99,"LR"), variable.name = "LR", value.name = "QVarID")
results = results_wide %>% melt(id.vars = 1:17, measure.vars = c(paste0("MT",1:99,".1_1"), paste0("MT",1:99,".1_2"), paste0("MP",1:99,".1_1"), paste0("MP",1:99,".1_2")), variable.name = "Question", value.name = "QID")
results = results_wide %>% melt(id.vars = 1:17, measure.vars = c(paste0("MT",1:99,".1_1"), paste0("MT",1:99,".1_2"), paste0("MP",1:99,".1_1"), paste0("MP",1:99,".1_2")), variable.name = "Question", value.name = "QID")
results = results_wide %>% melt(id.vars = 1:17, measure.vars = c(paste0("MT",1:99,".1_1"), paste0("MT",1:99,".1_2"), paste0("MP",1:99,".1_1"), paste0("MP",1:99,".1_2")), variable.name = "Question", value.name = "QID")
head(results)
results
View(results)
results = results_wide %>% melt(id.vars = 1:17, measure.vars = c(paste0("MT",1:99,".1_1"), paste0("MT",1:99,".1_2"), paste0("MP",1:99,".1_1"), paste0("MP",1:99,".1_2")), variable.name = "QID", value.name = "Choice", na.rm = T)
View(results)
results = results_wide %>% melt(id.vars = 1:17, measure.vars = c(paste0("MT",1:99,".1_1"), paste0("MT",1:99,".1_2"), paste0("MP",1:99,".1_1"), paste0("MP",1:99,".1_2")), variable.name = "QID", value.name = "Choice") %>% filter("Choice" != "")
View(results)
results$Choice
results$Choice == ""
results$Choice != ""
results = results_wide %>% melt(id.vars = 1:17, measure.vars = c(paste0("MT",1:99,".1_1"), paste0("MT",1:99,".1_2"), paste0("MP",1:99,".1_1"), paste0("MP",1:99,".1_2")), variable.name = "QID", value.name = "Choice") %>% filter("Choice" != "")
View(results)
stimuli = read.csv("Qualtrics_Full_List_of_Stimuli.csv", stringAsFactors = F)
?read.csv
stimuli = read.csv("Qualtrics_Full_List_of_Stimuli.csv", stringsAsFactors = F)
results = results_wide %>% melt(id.vars = 1:17, measure.vars = c(paste0("MT",1:99,".1_1"), paste0("MT",1:99,".1_2"), paste0("MP",1:99,".1_1"), paste0("MP",1:99,".1_2")), variable.name = "QID", value.name = "Choice") %>% filter("Choice" != "")
head(stimuli)
results = results_wide %>% melt(id.vars = 1:17, measure.vars = c(paste0("MT",1:99,".1_1"), paste0("MT",1:99,".1_2"), paste0("MP",1:99,".1_1"), paste0("MP",1:99,".1_2")), variable.name = "QID", value.name = "Choice") %>% filter("Choice" != "")
results_lr = results_wide %>% melt(id.vars = 1:17, measure.vars = c(paste0("MT",1:99,"LR"), paste0("MP",1:99,"LR"), variable.name = "LR", value.name = "QVarID")
results_lr = results_wide %>% melt(id.vars = 1:17, measure.vars = c(paste0("MT",1:99,"LR"), paste0("MP",1:99,"LR"), variable.name = "LR", value.name = "QVarID")
results_lr = results_wide %>% melt(id.vars = 1:17, measure.vars = c(paste0("MT",1:99,"LR"), paste0("MP",1:99,"LR"), variable.name = "LR", value.name = "QVarID")
results = results_wide %>% melt(id.vars = 1:17, measure.vars = c(paste0("MT",1:99,".1_1"), paste0("MT",1:99,".1_2"), paste0("MP",1:99,".1_1"), paste0("MP",1:99,".1_2")), variable.name = "QID", value.name = "Choice") %>% filter("Choice" != "")
results_lr = results_wide %>% melt(id.vars = 1:17, measure.vars = c(paste0("MT",1:99,"LR"), paste0("MP",1:99,"LR")), variable.name = "LR", value.name = "QVarID")
getInfoGainFromWordlist = function(feature.matrix, wordlist, freqs = NULL, mode = "naive", discrete = TRUE){
if(is.null(wordlist) | is.null(feature.matrix)){
stop("Information missing!")
}
#TODO - Some error-catching with feature matrices go here
#And some error catching with the wordlist, whatever can go wrong with it
if(!is.null(freqs)) {
if(length(wordlist) != length(freqs)){
stop("Worldist and frequencies must be identical in length.")
}
} else {
print("No frequencies provided. Assuming no frequency weighting.")
freqs = rep(1, length(wordlist))
}
#print(freqs)
#print(rownames(feature.matrix))
segments = rownames(feature.matrix)
i=1
segmentFreqs = numeric(length(segments))
names(segmentFreqs) = segments
for(entry in wordlist){
splitSegments = strsplit(entry,"")[[1]]
#print(segments)
#print(splitSegments)
if(!all(splitSegments %in% segments)){
unknownSegment = splitSegments[which(!(splitSegments %in% segments))]
stop(paste("Unknown segment found: ",unknownSegment," in string ",entry))
}
segmentFreqs[splitSegments] = segmentFreqs[splitSegments] + freqs[i]
i = i + 1
}
segmentProbs = segmentFreqs / sum(segmentFreqs)
#print(segmentFreqs)
#print(segmentProbs)
if(mode=="naive" & discrete){
baseEntropy = -sum(segmentProbs[segmentProbs>0] * log2(segmentProbs[segmentProbs>0]))
#print(paste("base",baseEntropy))
featureEntropy = numeric(ncol(feature.matrix))
for(i in 1:ncol(feature.matrix)){
curr.levels = levels(feature.matrix[,i])
remainingEntropies = numeric(length(curr.levels))
currSum = numeric(length(curr.levels))
print(paste("Reached",curr.levels))
for(j in 1:length(curr.levels)){
currFreqs = segmentFreqs[feature.matrix[,i]==curr.levels[j]]
#print("currFreqs:")
#print(currFreqs)
if(all(currFreqs == 0)){
next;
stop("Some feature values were never used! You may want to remove unused items from the alphabet.")
}
currProbs = currFreqs / sum(currFreqs)
#print(currProbs)
remainingEntropies[j] = -sum(currProbs[currProbs>0] * log2(currProbs[currProbs>0]))
#print(remainingEntropies[j])
currSum[j] = sum(segmentFreqs[which(feature.matrix[,i]==curr.levels[j])])
#print(paste("remainingEntropy",remainingEntropies[j]))
#print(paste("currSum",currSum[j]))
}
#print("newit")
#print(sum(remainingEntropies))
#print(sum(segmentFreqs))
#print(sum(currSum))
featureEntropy[i] = sum(remainingEntropies * currSum / sum(segmentFreqs))
#print(paste("featureEntropy",featureEntropy[j]))
}
#print(featureEntropy)
infoGain = baseEntropy - featureEntropy
names(infoGain) = colnames(feature.matrix)
} else if(mode == "broe" & discrete){
infoGain = numeric(ncol(feature.matrix))
names(infoGain) = colnames(feature.matrix)
pintersect_cum = 0
for(i in 1:ncol(feature.matrix)){
#print(colnames(feature.matrix)[i])
pintersect_cum = 0
num_cum = 0
reduced.feature.matrix = unique(feature.matrix)
for(j in 1:nrow(reduced.feature.matrix)){
if(as.character(feature.matrix[j,i]) != "0" & !is.na(feature.matrix[j,i])){
print("start")
pintersect = sum(segmentProbs[apply(compareRowToRows(as.matrix(reduced.feature.matrix[j,]),as.matrix(feature.matrix[,])),1,all)])
print(pintersect)
pcurr = sum(segmentProbs[apply(compareRowToRows(as.matrix(reduced.feature.matrix[j,i]),as.matrix(feature.matrix[,i])),1,all)])
prest = sum(segmentProbs[apply(compareRowToRows(as.matrix(reduced.feature.matrix[j,-i]),as.matrix(feature.matrix[,-i])),1,all)])
print(paste("pcurr",pcurr))
print(prest)
pintersect_cum = pintersect_cum + pintersect
print(pintersect_cum)
print(log2(1 / pcurr))
num_cum = num_cum + pintersect * log2(1 / pcurr)
print(paste("numcum",num_cum))
#print(num_cum)
}
}
infoGain[i] = num_cum / pintersect_cum
}
} else if(mode == "naive1" & discrete){ #A slow version of naive
infoGain = numeric(ncol(feature.matrix))
names(infoGain) = colnames(feature.matrix)
pintersect_cum = 0
for(i in 1:ncol(feature.matrix)){
num_cum = 0
reduced.feature.matrix = unique(feature.matrix)
for(j in 1:nrow(reduced.feature.matrix)){
pintersect = sum(segmentProbs[apply(compareRowToRows(as.matrix(reduced.feature.matrix[j,]),as.matrix(feature.matrix[,])),1,all)])
pcurr = sum(segmentProbs[apply(compareRowToRows(as.matrix(reduced.feature.matrix[j,i]),as.matrix(feature.matrix[,i])),1,all)])
prest = sum(segmentProbs[apply(compareRowToRows(as.matrix(reduced.feature.matrix[j,-i]),as.matrix(feature.matrix[,-i])),1,all)])
#print("start")
#print(pintersect)
#print(pcurr)
#print(prest)
#print(pintersect_cum)
#print(log2(pintersect / pcurr / prest))
num_cum = num_cum + pintersect * log2(1 / pcurr)
#print(num_cum)
}
infoGain[i] = num_cum
}
} else if(mode == "naive" & !discrete){
#todo: tokens instead of types
require(mclust)
require(mvtnorm)
wordlist.tokens = Reduce(c, lapply(1:nrow(lexicon), function(x) rep(wordlist[x], freqs[x])))
wordlist.feat = feature.matrix[Reduce(c, strsplit(wordlist.tokens, "")),]
feat.gmm = densityMclust(wordlist.feat)
feat.gmm.means = feat.gmm$parameters$mean
feat.gmm.vars = feat.gmm$parameters$variance$sigma
feat.gmm.probs = feat.gmm$parameters$pro
infoGain =  as.vector(simulateInfoGainFromGMM(feat.gmm.means, feat.gmm.vars, feat.gmm.probs))
names(infoGain) = names(feature.matrix)
}
return(infoGain)
}
load("G:/我的雲端硬碟/Phonotactics/judgement-experiment/distance data/may28-secondlast.RData")
multivalue.weights.cont = getInfoGainFromWordlist(feature.matrix.multivalue,allophones(lexicon$klatt),lexicon$freq,mode="naive", discrete = F)
euclidean_segdist_multivalue_weighted_cont = diag(stringdist(old_disyl_segments_allophones,tostrings=new_disyl_segments_allophones,segment.specific.costs=TRUE,distance.matrix=distances.multivalue.euclidean,costs=c(.5,.5,1,1),weights=multivalue.weights.cont))
manhattan_segdist_multivalue_weighted_cont = diag(stringdist(old_disyl_segments_allophones,tostrings=new_disyl_segments_allophones,segment.specific.costs=TRUE,distance.matrix=distances.multivalue.manhattan,costs=c(.5,.5,1,1),weights=multivalue.weights.cont))
hamming_segdist_multivalue_weighted_cont = diag(stringdist(old_disyl_segments_allophones,tostrings=new_disyl_segments_allophones,segment.specific.costs=TRUE,distance.matrix=distances.multivalue.hamming,costs=c(.5,.5,1,1),weights=multivalue.weights.cont))
euclidean_segdist_multivalue_weighted_cont = cbind.data.frame(paste("D",seq(1,72,1),"_1",sep=""),euclidean_segdist_multivalue_weighted_cont)
colnames(euclidean_segdist_multivalue_weighted_cont)[1] = "variable"
manhattan_segdist_multivalue_weighted_cont = cbind.data.frame(paste("D",seq(1,72,1),"_1",sep=""),manhattan_segdist_multivalue_weighted_cont)
colnames(manhattan_segdist_multivalue_weighted_cont)[1] = "variable"
hamming_segdist_multivalue_weighted_cont = cbind.data.frame(paste("D",seq(1,72,1),"_1",sep=""),hamming_segdist_multivalue_weighted_cont)
colnames(hamming_segdist_multivalue_weighted_cont)[1] = "variable"
dist_judgements_melted_di = right_join(dist_judgements_melted_di, euclidean_segdist_multivalue_weighted_cont, by = "variable")
dist_judgements_melted_di = right_join(dist_judgements_melted_di, manhattan_segdist_multivalue_weighted_cont, by = "variable")
dist_judgements_melted_di = right_join(dist_judgements_melted_di, hamming_segdist_multivalue_weighted_cont, by = "variable")
library(dplyr)
dist_judgements_melted_di = right_join(dist_judgements_melted_di, euclidean_segdist_multivalue_weighted_cont, by = "variable")
dist_judgements_melted_di = right_join(dist_judgements_melted_di, manhattan_segdist_multivalue_weighted_cont, by = "variable")
dist_judgements_melted_di = right_join(dist_judgements_melted_di, hamming_segdist_multivalue_weighted_cont, by = "variable")
model_di_hamming_auto_euclidean_seg_weighted_cont = brm(distance | cens(censored_di) ~ (euclidean_segdist_multivalue_weighted_cont + hamming_auto_weighted | participant) +(1|variable) + euclidean_segdist_multivalue_weighted_cont + hamming_auto_weighted, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
model_di_hamming_oc_euclidean_seg_weighted_cont = brm(distance | cens(censored_di) ~ (euclidean_segdist_multivalue_weighted_cont + hamming_oc_weighted | participant) +(1|variable) + euclidean_segdist_multivalue_weighted_cont + hamming_oc_weighted, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
model_di_hamming_oco_euclidean_seg_weighted_cont = brm(distance | cens(censored_di) ~ (euclidean_segdist_multivalue_weighted_cont + hamming_oco_weighted | participant) +(1|variable) + euclidean_segdist_multivalue_weighted_cont + hamming_oco_weighted, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
model_di_hamming_co_euclidean_seg_weighted_cont = brm(distance | cens(censored_di) ~ (euclidean_segdist_multivalue_weighted_cont + hamming_co_weighted | participant) +(1|variable) + euclidean_segdist_multivalue_weighted_cont + hamming_co_weighted, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
model_di_hamming_chao_euclidean_seg_weighted_cont = brm(distance | cens(censored_di) ~ (euclidean_segdist_multivalue_weighted_cont + hamming_chao_weighted | participant) +(1|variable) + euclidean_segdist_multivalue_weighted_cont + hamming_chao_weighted, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
model_di_man_chao_euclidean_seg_weighted_cont = brm(distance | cens(censored_di) ~ (euclidean_segdist_multivalue_weighted_cont + man_chao_weighted | participant) +(1|variable) + euclidean_segdist_multivalue_weighted_cont + man_chao_weighted, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
model_di_euc_chao_euclidean_seg_weighted_cont = brm(distance | cens(censored_di) ~ (euclidean_segdist_multivalue_weighted_cont + euc_chao_weighted | participant) +(1|variable) + euclidean_segdist_multivalue_weighted_cont + euc_chao_weighted, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
waic(model_di_hamming_auto_euclidean_seg_weighted_cont)
waic(model_di_hamming_oc_euclidean_seg_weighted_cont)
waic(model_di_hamming_oco_euclidean_seg_weighted_cont)
waic(model_di_hamming_co_euclidean_seg_weighted_cont)
waic(model_di_hamming_chao_euclidean_seg_weighted_cont)
waic(model_di_man_chao_euclidean_seg_weighted_cont)
waic(model_di_euc_chao_euclidean_seg_weighted_cont)
hamming_di_euclidean_weighted_seg_kfold = kfold(model_di_hamming_auto_euclidean_seg_weighted_cont ,model_di_hamming_oc_euclidean_seg_weighted_cont ,model_di_hamming_oco_euclidean_seg_weighted_cont ,model_di_hamming_co_euclidean_seg_weighted_cont ,model_di_hamming_chao_euclidean_seg_weighted_cont ,model_di_man_chao_euclidean_seg_weighted_cont ,model_di_euc_chao_euclidean_seg_weighted_cont , compare = TRUE, K = 10, Ksub = NULL, folds = NULL, group = NULL, exact_loo = NULL, resp = NULL, model_names = NULL, save_fits = FALSE)
library(brms)
model_di_hamming_auto_euclidean_seg_weighted_cont = brm(distance | cens(censored_di) ~ (euclidean_segdist_multivalue_weighted_cont + hamming_auto_weighted | participant) +(1|variable) + euclidean_segdist_multivalue_weighted_cont + hamming_auto_weighted, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
model_di_hamming_oc_euclidean_seg_weighted_cont = brm(distance | cens(censored_di) ~ (euclidean_segdist_multivalue_weighted_cont + hamming_oc_weighted | participant) +(1|variable) + euclidean_segdist_multivalue_weighted_cont + hamming_oc_weighted, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
model_di_hamming_oco_euclidean_seg_weighted_cont = brm(distance | cens(censored_di) ~ (euclidean_segdist_multivalue_weighted_cont + hamming_oco_weighted | participant) +(1|variable) + euclidean_segdist_multivalue_weighted_cont + hamming_oco_weighted, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
model_di_hamming_co_euclidean_seg_weighted_cont = brm(distance | cens(censored_di) ~ (euclidean_segdist_multivalue_weighted_cont + hamming_co_weighted | participant) +(1|variable) + euclidean_segdist_multivalue_weighted_cont + hamming_co_weighted, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
model_di_hamming_chao_euclidean_seg_weighted_cont = brm(distance | cens(censored_di) ~ (euclidean_segdist_multivalue_weighted_cont + hamming_chao_weighted | participant) +(1|variable) + euclidean_segdist_multivalue_weighted_cont + hamming_chao_weighted, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
model_di_man_chao_euclidean_seg_weighted_cont = brm(distance | cens(censored_di) ~ (euclidean_segdist_multivalue_weighted_cont + man_chao_weighted | participant) +(1|variable) + euclidean_segdist_multivalue_weighted_cont + man_chao_weighted, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
model_di_euc_chao_euclidean_seg_weighted_cont = brm(distance | cens(censored_di) ~ (euclidean_segdist_multivalue_weighted_cont + euc_chao_weighted | participant) +(1|variable) + euclidean_segdist_multivalue_weighted_cont + euc_chao_weighted, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
waic(model_di_hamming_auto_euclidean_seg_weighted_cont)
waic(model_di_hamming_oc_euclidean_seg_weighted_cont)
waic(model_di_hamming_oco_euclidean_seg_weighted_cont)
waic(model_di_hamming_co_euclidean_seg_weighted_cont)
waic(model_di_hamming_chao_euclidean_seg_weighted_cont)
waic(model_di_man_chao_euclidean_seg_weighted_cont)
waic(model_di_euc_chao_euclidean_seg_weighted_cont)
hamming_di_euclidean_weighted_seg_kfold = kfold(model_di_hamming_auto_euclidean_seg_weighted_cont ,model_di_hamming_oc_euclidean_seg_weighted_cont ,model_di_hamming_oco_euclidean_seg_weighted_cont ,model_di_hamming_co_euclidean_seg_weighted_cont ,model_di_hamming_chao_euclidean_seg_weighted_cont ,model_di_man_chao_euclidean_seg_weighted_cont ,model_di_euc_chao_euclidean_seg_weighted_cont , compare = TRUE, K = 10, Ksub = NULL, folds = NULL, group = NULL, exact_loo = NULL, resp = NULL, model_names = NULL, save_fits = FALSE)
model_di_hamming_auto_manhattan_seg_weighted_cont = brm(distance | cens(censored_di) ~ (manhattan_segdist_multivalue_weighted_cont + hamming_auto_weighted | participant) +(1|variable) + manhattan_segdist_multivalue_weighted_cont + hamming_auto_weighted, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
model_di_hamming_oc_manhattan_seg_weighted_cont = brm(distance | cens(censored_di) ~ (manhattan_segdist_multivalue_weighted_cont + hamming_oc_weighted | participant) +(1|variable) + manhattan_segdist_multivalue_weighted_cont + hamming_oc_weighted, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
model_di_hamming_oco_manhattan_seg_weighted_cont = brm(distance | cens(censored_di) ~ (manhattan_segdist_multivalue_weighted_cont + hamming_oco_weighted | participant) +(1|variable) + manhattan_segdist_multivalue_weighted_cont + hamming_oco_weighted, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
model_di_hamming_co_manhattan_seg_weighted_cont = brm(distance | cens(censored_di) ~ (manhattan_segdist_multivalue_weighted_cont + hamming_co_weighted | participant) +(1|variable) + manhattan_segdist_multivalue_weighted_cont + hamming_co_weighted, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
model_di_hamming_chao_manhattan_seg_weighted_cont = brm(distance | cens(censored_di) ~ (manhattan_segdist_multivalue_weighted_cont + hamming_chao_weighted | participant) +(1|variable) + manhattan_segdist_multivalue_weighted_cont + hamming_chao_weighted, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
model_di_man_chao_manhattan_seg_weighted_cont = brm(distance | cens(censored_di) ~ (manhattan_segdist_multivalue_weighted_cont + man_chao_weighted | participant) +(1|variable) + manhattan_segdist_multivalue_weighted_cont + man_chao_weighted, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
model_di_euc_chao_manhattan_seg_weighted_cont = brm(distance | cens(censored_di) ~ (manhattan_segdist_multivalue_weighted_cont + euc_chao_weighted | participant) +(1|variable) + manhattan_segdist_multivalue_weighted_cont + euc_chao_weighted, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
waic(model_di_hamming_auto_manhattan_seg_weighted_cont)
waic(model_di_hamming_oc_manhattan_seg_weighted_cont)
waic(model_di_hamming_oco_manhattan_seg_weighted_cont)
waic(model_di_hamming_co_manhattan_seg_weighted_cont)
waic(model_di_hamming_chao_manhattan_seg_weighted_cont)
waic(model_di_man_chao_manhattan_seg_weighted_cont)
waic(model_di_euc_chao_manhattan_seg_weighted_cont)
hamming_di_manhattan_weighted_seg_kfold = kfold(model_di_hamming_auto_manhattan_seg_weighted_cont ,model_di_hamming_oc_manhattan_seg_weighted_cont ,model_di_hamming_oco_manhattan_seg_weighted_cont ,model_di_hamming_co_manhattan_seg_weighted_cont ,model_di_hamming_chao_manhattan_seg_weighted_cont ,model_di_man_chao_manhattan_seg_weighted_cont ,model_di_euc_chao_manhattan_seg_weighted_cont , compare = TRUE, K = 10, Ksub = NULL, folds = NULL, group = NULL, exact_loo = NULL, resp = NULL, model_names = NULL, save_fits = FALSE)
waic(model_di_man_chao_manhattan_seg_weighted_cont)
waic(model_di_man_chao_manhattan_seg_weighted)
waic(model_di_hamming_chao_euclidean_seg_weighted_cont)
waic(model_di_hamming_chao_euclidean_seg_weighted)
waic(model_di_hamming_auto_euclidean_seg_weighted)
waic(model_di_hamming_oc_euclidean_seg_weighted)
waic(model_di_hamming_oco_euclidean_seg_weighted)
waic(model_di_hamming_co_euclidean_seg_weighted)
waic(model_di_hamming_chao_euclidean_seg_weighted)
waic(model_di_man_chao_euclidean_seg_weighted)
waic(model_di_euc_chao_euclidean_seg_weighted)
hamming_di_euclidean_weighted_seg_kfold
hamming_di_manhattan_weighted_seg_kfold
waic(model_di_hamming_auto_manhattan_seg)
waic(model_di_hamming_oc_manhattan_seg)
waic(model_di_hamming_oco_manhattan_seg)
waic(model_di_hamming_co_manhattan_seg)
waic(model_di_hamming_chao_manhattan_seg)
waic(model_di_man_chao_manhattan_seg)
waic(model_di_euc_chao_manhattan_seg)
#MFCC
findDisylFormantDistance = function(disyl){
oldfile = rbind(read.table(paste("G:\\我的雲端硬碟\\Phonotactics\\judgement-experiment\\distance audio\\sepaudio\\formants\\normalised_DISYL-OLD-",disyl,"_formants_1.txt",sep="")),read.table(paste("G:\\我的雲端硬碟\\Phonotactics\\judgement-experiment\\distance audio\\sepaudio\\formants\\normalised_DISYL-OLD-",disyl,"_formants_2.txt",sep="")),read.table(paste("G:\\我的雲端硬碟\\Phonotactics\\judgement-experiment\\distance audio\\sepaudio\\formants\\normalised_DISYL-OLD-",disyl,"_formants_2.txt",sep="")))
newfile = rbind(read.table(paste("G:\\我的雲端硬碟\\Phonotactics\\judgement-experiment\\distance audio\\sepaudio\\formants\\normalised_DISYL-NEW-",disyl,"_formants_1.txt",sep="")),read.table(paste("G:\\我的雲端硬碟\\Phonotactics\\judgement-experiment\\distance audio\\sepaudio\\formants\\normalised_DISYL-NEW-",disyl,"_formants_2.txt",sep="")),read.table(paste("G:\\我的雲端硬碟\\Phonotactics\\judgement-experiment\\distance audio\\sepaudio\\formants\\normalised_DISYL-NEW-",disyl,"_formants_3.txt",sep="")))
return(spectrumDistance(oldfile,newfile))
}
formants_dists_disyl = sapply(1:72,findDisylFormantDistance)
dist_judgements_melted_di = addDistsToDiTable(formants_dists_disyl,"formants_acoustic_dist")
formants_dists_disyl_2 = formants_dists_disyl / max(formants_dists_disyl) * 4
dist_judgements_melted_di = addDistsToDiTable(formants_dists_disyl_2,"formants_acoustic_dist_modified")
model_di_acoust_formants = brm(distance | cens(censored_di) ~ (formants_acoustic_dist_modified | participant) +(1|variable) + formants_acoustic_dist_modified, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
72/12
4/6
#MFCC
findDisylFormantDistance = function(disyl){
oldfile = rbind(read.table(paste("G:\\我的雲端硬碟\\Phonotactics\\judgement-experiment\\distance audio\\sepaudio\\formants\\normalised_DISYL-OLD-",ceiling(disyl/6),"_formants_1.txt",sep="")),read.table(paste("G:\\我的雲端硬碟\\Phonotactics\\judgement-experiment\\distance audio\\sepaudio\\formants\\normalised_DISYL-OLD-",ceiling(disyl/6),"_formants_2.txt",sep="")),read.table(paste("G:\\我的雲端硬碟\\Phonotactics\\judgement-experiment\\distance audio\\sepaudio\\formants\\normalised_DISYL-OLD-",ceiling(disyl/6),"_formants_2.txt",sep="")))
newfile = rbind(read.table(paste("G:\\我的雲端硬碟\\Phonotactics\\judgement-experiment\\distance audio\\sepaudio\\formants\\normalised_DISYL-NEW-",disyl,"_formants_1.txt",sep="")),read.table(paste("G:\\我的雲端硬碟\\Phonotactics\\judgement-experiment\\distance audio\\sepaudio\\formants\\normalised_DISYL-NEW-",disyl,"_formants_2.txt",sep="")),read.table(paste("G:\\我的雲端硬碟\\Phonotactics\\judgement-experiment\\distance audio\\sepaudio\\formants\\normalised_DISYL-NEW-",disyl,"_formants_3.txt",sep="")))
return(spectrumDistance(oldfile,newfile))
}
formants_dists_disyl = sapply(1:72,findDisylFormantDistance)
dist_judgements_melted_di = addDistsToDiTable(formants_dists_disyl,"formants_acoustic_dist")
formants_dists_disyl_2 = formants_dists_disyl / max(formants_dists_disyl) * 4
dist_judgements_melted_di = addDistsToDiTable(formants_dists_disyl_2,"formants_acoustic_dist_modified")
model_di_acoust_formants = brm(distance | cens(censored_di) ~ (formants_acoustic_dist_modified | participant) +(1|variable) + formants_acoustic_dist_modified, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
#MFCC
findDisylMFCCDistance = function(disyl){
oldfile = read.table(paste("G:\\我的雲端硬碟\\Phonotactics\\judgement-experiment\\distance audio\\sepaudio\\mfcc\\normalised_DISYL-OLD-",disyl,"_mfcc.txt",sep=""))
newfile = read.table(paste("G:\\我的雲端硬碟\\Phonotactics\\judgement-experiment\\distance audio\\sepaudio\\mfcc\\normalised_DISYL-NEW-",disyl,"_mfcc.txt",sep=""))
return(spectrumDistance(oldfile,newfile))
}
mfcc_dists_disyl = sapply(1:72,findDisylMFCCDistance)
dist_judgements_melted_di = addDistsToDiTable(mfcc_dists_disyl,"mfcc_acoustic_dist")
mfcc_dists_disyl_2 = mfcc_dists_disyl / max(mfcc_dists_disyl) * 4
dist_judgements_melted_di = addDistsToDiTable(mfcc_dists_disyl_2,"mfcc_acoustic_dist_modified")
model_di_acoust_mfcc = brm(distance | cens(censored_di) ~ (mfcc_acoustic_dist_modified | participant) +(1|variable) + mfcc_acoustic_dist_modified, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
findDisylMFCCDistance = function(disyl){
oldfile = read.table(paste("G:\\我的雲端硬碟\\Phonotactics\\judgement-experiment\\distance audio\\sepaudio\\mfcc\\normalised_DISYL-OLD-",ceiling(disyl/6),"_mfcc.txt",sep=""))
newfile = read.table(paste("G:\\我的雲端硬碟\\Phonotactics\\judgement-experiment\\distance audio\\sepaudio\\mfcc\\normalised_DISYL-NEW-",disyl,"_mfcc.txt",sep=""))
return(spectrumDistance(oldfile,newfile))
}
mfcc_dists_disyl = sapply(1:72,findDisylMFCCDistance)
dist_judgements_melted_di = addDistsToDiTable(mfcc_dists_disyl,"mfcc_acoustic_dist")
mfcc_dists_disyl_2 = mfcc_dists_disyl / max(mfcc_dists_disyl) * 4
dist_judgements_melted_di = addDistsToDiTable(mfcc_dists_disyl_2,"mfcc_acoustic_dist_modified")
model_di_acoust_mfcc = brm(distance | cens(censored_di) ~ (mfcc_acoustic_dist_modified | participant) +(1|variable) + mfcc_acoustic_dist_modified, data = dist_judgements_melted_di, cores = getOption("mc.cores", 4L))
waic(model_di_acoust_mfcc)
waic(dist_judgements_melted_di)
waic(model_di_acoust_formants)
save.image("G:/我的雲端硬碟/Phonotactics/judgement-experiment/distance data/20190827-formants.RData")
